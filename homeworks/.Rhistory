valid_datagen <- image_data_generator(
rescale = 1/255
)
train_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/train/',
target_size = c(128, 128),
generator = train_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
#shuffle = TRUE
)
valid_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/test/',
target_size = c(128, 128),
generator = valid_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
#shuffle = TRUE
)
batch_size <- 16
# image augmentation
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.3,
height_shift_range = 0.3,
shear_range = 0.3,
zoom_range = 0.3,
horizontal_flip=TRUE,
fill_mode='nearest'
)
valid_datagen <- image_data_generator(
rescale = 1/255
)
train_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/train/',
target_size = c(128, 128),
generator = train_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
#shuffle = TRUE
)
valid_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/test/',
target_size = c(128, 128),
generator = valid_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
#shuffle = TRUE
)
hd_cnn_aug <- hd_cnn
compile(
hd_cnn_aug,
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_cnn_aug.h5', monitor = 'val_loss', save_best_only = TRUE)
history_hot_dog_aug <- fit(
hd_cnn_aug,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=valid_generator$n/batch_size,
steps_per_epoch=train_generator$n/batch_size,
callback = c(early_stop, checkpoint)
)
png('history_hot_dog_aug.png', width = 800, height = 600)
plot(history_hot_dog_aug)
invisible(dev.off())
png('history_hot_dog_aug.png', width = 800, height = 600)
plot(history_hot_dog_aug)
invisible(dev.off())
best_hd_cnn_aug <- keras::load_model_hdf5('models/best_hd_cnn_aug.h5')
knitr::include_graphics('history_hot_dog_aug.png')
evaluate_generator(best_hd_cnn_aug, valid_generator, steps = valid_generator$n/batch_size)
#download VGG16 model
conv_base <- application_vgg16(weights='imagenet', include_top= FALSE, input_shape=c(128,128,3))
summary(conv_base)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 16, activation = 'relu') |>
layer_dense(units = 1, activation = 'sigmoid')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 16, activation = 'relu') |>
layer_dense(units = 1, activation = 'sigmoid')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base)
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 10,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size,
callbacks = c(early_stop, checkpoint)
)
batch_size <- 16
# image augmentation
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.3,
height_shift_range = 0.3,
shear_range = 0.3,
zoom_range = 0.3,
horizontal_flip=TRUE,
fill_mode='nearest'
)
valid_datagen <- image_data_generator(
rescale = 1/255
)
train_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/train/',
target_size = c(128, 128),
generator = train_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
shuffle = TRUE
)
valid_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/test/',
target_size = c(128, 128),
generator = valid_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
shuffle = TRUE
)
batch_size <- 16
# image augmentation
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.3,
height_shift_range = 0.3,
shear_range = 0.3,
zoom_range = 0.3,
horizontal_flip=TRUE,
fill_mode='nearest'
)
valid_datagen <- image_data_generator(
rescale = 1/255
)
train_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/train/',
target_size = c(128, 128),
generator = train_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
shuffle = TRUE
)
valid_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/test/',
target_size = c(128, 128),
generator = valid_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb"
)
hd_cnn_aug <- hd_cnn
compile(
hd_cnn_aug,
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = c('accuracy')
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_cnn_aug.h5', monitor = 'val_loss', save_best_only = TRUE)
history_hot_dog_aug <- fit(
hd_cnn_aug,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=valid_generator$n/batch_size,
steps_per_epoch=train_generator$n/batch_size,
callback = c(early_stop, checkpoint)
)
compile(
hd_cnn_aug,
loss = 'binary_crossentropy',
optimizer = optimizer_adam(learning_rate = 1e-6),
metrics = c('accuracy')
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_cnn_aug.h5', monitor = 'val_loss', save_best_only = TRUE)
history_hot_dog_aug <- fit(
hd_cnn_aug,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=valid_generator$n/batch_size,
steps_per_epoch=train_generator$n/batch_size,
callback = c(early_stop, checkpoint)
)
batch_size <- 16
# image augmentation
train_datagen <- image_data_generator(
rescale = 1/255,
rotation_range = 40,
width_shift_range = 0.3,
height_shift_range = 0.3,
shear_range = 0.3,
zoom_range = 0.3,
horizontal_flip=TRUE,
fill_mode='nearest'
)
valid_datagen <- image_data_generator(
rescale = 1/255
)
train_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/train/',
target_size = c(128, 128),
generator = train_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb",
shuffle = TRUE
)
valid_generator <- flow_images_from_directory(
'../data/hot-dog-not-hot-dog/test/',
target_size = c(128, 128),
generator = valid_datagen,
batch_size = batch_size,
class_mode = 'binary',
classes = c('hot_dog', 'not_hot_dog'),
color_mode="rgb"
)
hd_cnn_aug <- hd_cnn
compile(
hd_cnn_aug,
loss = 'binary_crossentropy',
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = c('accuracy')
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_cnn_aug.h5', monitor = 'val_loss', save_best_only = TRUE)
history_hot_dog_aug <- fit(
hd_cnn_aug,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=valid_generator$n/batch_size,
steps_per_epoch=train_generator$n/batch_size,
callback = c(early_stop, checkpoint)
)
compile(
hd_cnn_aug,
loss = 'binary_crossentropy',
optimizer = optimizer_adam(learning_rate = 1e-6),
metrics = c('accuracy')
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_cnn_aug.h5', monitor = 'val_loss', save_best_only = TRUE)
```{r}
history_hot_dog_aug <- fit(
hd_cnn_aug,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=valid_generator$n/batch_size,
steps_per_epoch=train_generator$n/batch_size
#callback = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 128, activation = 'relu') |>
layer_dense(units = 2, activation = 'softmax')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 1e-6),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
```{r freeze-params}
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base)
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 128, activation = 'relu') |>
layer_dense(units = 1, activation = 'softmax')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 1e-6),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
```{r freeze-params}
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base)
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 128, activation = 'relu') |>
layer_dense(units = 1, activation = 'softmax')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 256, activation = 'relu') |>
layer_dropout(0.5) |>
layer_dense(units = 1, activation = 'softmax')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base)
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 256, activation = 'relu') |>
layer_dropout(0.5) |>
layer_dense(units = 1, activation = 'softmax')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_sgd(learning_rate = 0.001),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base)
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 30,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 256, activation = 'relu') |>
layer_dropout(0.5) |>
layer_dense(units = 1, activation = 'sigmoid')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_sgd(learning_rate = 0.001),
metrics = c("accuracy")
)
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base, to = 'block5_conv1')
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 100,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
transfer_model <- keras_model_sequential() |>
conv_base() |>
layer_flatten() |>
layer_dense(units = 256, activation = 'relu') |>
layer_dropout(0.5) |>
layer_dense(units = 1, activation = 'sigmoid')
compile(
transfer_model,
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.000001),
metrics = c("accuracy")
)
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=10)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/best_hd_transfer.h5', monitor = 'val_loss', save_best_only = TRUE)
#freeze weights of the transfer model (why would we need to re-estimate again)
freeze_weights(conv_base, to = 'block5_conv1')
summary(transfer_model)
history_hot_dog_trans <- fit(
transfer_model,
train_generator,
epochs = 100,
validation_data = valid_generator,
validation_steps=length(valid_generator$labels)/batch_size,
steps_per_epoch=length(train_generator$labels)/batch_size
#callbacks = c(early_stop, checkpoint)
)
