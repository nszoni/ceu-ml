train = calculateAccuracy(ifelse(predict(rf, train)$predictions < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(predict(rf, valid)$predictions < 0.5, 0, 1), valid$is_popular)
)
show_table()
#predict holdout
article_ids <- holdout$article_id
to_drop <- c("n_non_stop_unique_tokens","n_non_stop_words","kw_avg_min","article_id","timedelta")
# drop useless columns which highly correlate or is not informative
sub <- select(holdout, -all_of(to_drop))
rf_score <- predict(rf, data=sub)$predictions
rf_submission <-data.frame(article_id = article_ids, score = rf_score)
write_csv(rf_submission, "./submissions/rf_submission.csv")
#define predictor and response variables in training set
train_x <- data.matrix(train[, 1:55])
train_y <- train$is_popular
#define predictor and response variables in testing set
valid_x <- data.matrix(valid[, 1:55])
valid_y <- valid$is_popular
#define final training and testing sets
xgb_train <- xgb.DMatrix(data = train_x, label = train_y)
xgb_test <- xgb.DMatrix(data = valid_x, label = valid_y)
# train a model using our training data
xgb <- xgboost(data = xgb_train,
max.depth = 7,
nround = 300,
eta = 0.05, #learning rate
eval_metric = 'auc',
early_stopping_rounds = 10,
objective = "binary:logistic")
xgb_valid <- xgb.DMatrix(data = valid_x, label = valid_y)
accuracy_results <- add_row(accuracy_results,
model = "XGBoost",
train = calculateAccuracy(ifelse(predict(xgb, xgb_train) < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(predict(xgb, xgb_test) < 0.5, 0, 1), valid$is_popular)
)
show_table()
#predict holdout
article_ids <- holdout$article_id
to_drop <- c("n_non_stop_unique_tokens","n_non_stop_words","kw_avg_min","article_id","timedelta")
sub <- select(holdout, -all_of(to_drop))
xgb_score <- predict(xgb, xgb.DMatrix(as.matrix(sub)))
xgb_submission <-data.frame(article_id = article_ids, score = xgb_score)
write_csv(xgb_submission, "./submissions/xgb_submission.csv")
# Separate x & rescale
data_train_x <- as.matrix(select(train, -is_popular))
data_valid_x <- as.matrix(select(valid, -is_popular))
# Separate y & one-hot encoding
data_train_y <- to_categorical(train$is_popular, 2)
# Separate y & one-hot encoding
data_train_y <- to_categorical(train$is_popular, 2)
data_valid_y <- to_categorical(valid$is_popular, 2)
news_nn <- keras_model_sequential()
news_nn %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 32, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 2, activation = "softmax")
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=3)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/online_news_nn.h5', monitor = 'val_loss', save_best_only = TRUE)
summary(news_nn)
news_nn |> compile(
loss = "binary_crossentropy",
optimizer = optimizer_sgd(learning_rate = 1e-5),
metrics = c("accuracy")
)
history <- fit(
news_nn, data_train_x, data_train_y,
epochs = 15, batch_size = 5,
validation_data = list(data_valid_x, data_valid_y),
callbacks = c(early_stop, checkpoint)
)
history <- fit(
news_nn, data_train_x, data_train_y,
epochs = 15, batch_size = 30,
validation_data = list(data_valid_x, data_valid_y),
callbacks = c(early_stop, checkpoint)
)
best_nn <- keras::load_model_hdf5('models/online_news_nn.h5')
plot(history)
news_nn <- keras_model_sequential()
news_nn %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 2, activation = "softmax")
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=3)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/online_news_nn.h5', monitor = 'val_loss', save_best_only = TRUE)
summary(news_nn)
news_nn |> compile(
loss = "binary_crossentropy",
optimizer = optimizer_sgd(learning_rate = 1e-5),
metrics = c("accuracy")
)
history <- fit(
news_nn, data_train_x, data_train_y,
epochs = 15, batch_size = 30,
validation_data = list(data_valid_x, data_valid_y),
callbacks = c(early_stop, checkpoint)
)
plot(history)
best_nn <- keras::load_model_hdf5('models/online_news_nn.h5')
predict(best_nn, data_valid_x, batch_size = 30)
# Separate y & one-hot encoding
data_train_y <- to_categorical(train$is_popular, 1)
# Separate x & rescale
data_train_x <- as.matrix(select(train, -is_popular))
data_valid_x <- as.matrix(select(valid, -is_popular))
# Separate y & one-hot encoding
data_train_y <- train$is_popular
data_valid_y <- valid$is_popular
news_nn <- keras_model_sequential()
news_nn %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 1, activation = "sigmoid")
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=3)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/online_news_nn.h5', monitor = 'val_loss', save_best_only = TRUE)
summary(news_nn)
news_nn |> compile(
loss = "binary_crossentropy",
optimizer = optimizer_sgd(learning_rate = 1e-5),
metrics = c("accuracy")
)
history <- fit(
news_nn, data_train_x, data_train_y,
epochs = 15, batch_size = 30,
validation_data = list(data_valid_x, data_valid_y),
callbacks = c(early_stop, checkpoint)
)
plot(history)
best_nn <- keras::load_model_hdf5('models/online_news_nn.h5')
predict(best_nn, data_train_x, batch_size = 30)
summary(predict(best_nn, data_train_x, batch_size = 30))
table(predict(best_nn, data_train_x, batch_size = 30))
accuracy_results <- add_row(accuracy_results,
model = "Neural Net",
train = calculateAccuracy(ifelse(predict(best_nn, data_train_x, batch_size = 30) < 0.5, 0, 1), data_train_y),
valid = calculateAccuracy(ifelse(predict(best_nn, data_valid_x, batch_size = 30) < 0.5, 0, 1), data_valid_y)
)
accuracy_results <- add_row(accuracy_results,
model = "Neural Net",
train = calculateAccuracy(ifelse(predict(best_nn, data_train_x, batch_size = 30) < 0.5, 0, 1), data_train_y),
test = calculateAccuracy(ifelse(predict(best_nn, data_valid_x, batch_size = 30) < 0.5, 0, 1), data_valid_y)
)
show_table()
news_nn |> compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = 1e-5),
metrics = c("accuracy")
)
history <- fit(
news_nn, data_train_x, data_train_y,
epochs = 15, batch_size = 30,
validation_data = list(data_valid_x, data_valid_y),
callbacks = c(early_stop, checkpoint)
)
best_nn <- keras::load_model_hdf5('models/online_news_nn.h5')
accuracy_results <- add_row(accuracy_results,
model = "Neural Net",
train = calculateAccuracy(ifelse(predict(best_nn, data_train_x, batch_size = 30) < 0.5, 0, 1), data_train_y),
test = calculateAccuracy(ifelse(predict(best_nn, data_valid_x, batch_size = 30) < 0.5, 0, 1), data_valid_y)
)
show_table()
evaluate(best_nn, data_valid_x, data_valid_y)
show_table()
nn_score <- predict(best_nn, sub, batch_size = 30, verbose=1)
test_nn <- as.matrix(select(sub))
nn_score <- predict(best_nn, test_nn, batch_size = 30, verbose=1)
test_nn <- as.matrix(sub)
nn_score <- predict(best_nn, test_nn, batch_size = 30, verbose=1)
nn_score
table(nn_score)
nn_submission <-data.frame(article_id = article_ids, score = nn_score)
write_csv(nn_submission, "./submissions/nn_submission.csv")
news_nn <- keras_model_sequential()
news_nn %>%
layer_dense(units = 64, activation = "relu", input_shape = c(ncol(data_train_x))) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = "sigmoid")
#define early stopping callback
early_stop <- keras::callback_early_stopping(monitor = 'val_loss', patience=3)
#define checkpoint callback
checkpoint <- keras::callback_model_checkpoint(filepath = 'models/online_news_nn.h5', monitor = 'val_loss', save_best_only = TRUE)
summary(news_nn)
logit_score <- predict(logit, sub, type='response')
lasso_score <- c(predict(lasso, newx=model.matrix(~., sub)[,-1]))
ridge_score <- c(predict(ridge, newx=model.matrix(~., sub)[,-1]))
rf_score <- predict(rf, data=sub)$predictions
#fit model
ridge <- cv.glmnet(X, Y, alpha=0, nlambda=200)
formula <- as.formula(is_popular ~ .)
X = model.matrix(formula, train)[,-1]
Y = train$is_popular
logit_score <- predict(logit, sub, type='response')
lasso_score <- c(predict(lasso, newx=model.matrix(~., sub)[,-1]))
ridge_score <- c(predict(ridge, newx=model.matrix(~., sub)[,-1]))
#fit model
ridge <- cv.glmnet(X, Y, alpha=0, nlambda=200)
ridge_score <- c(predict(ridge, newx=model.matrix(~., sub)[,-1]))
rf_score <- predict(rf, data=sub)$predictions
xg_score <- predict(xgb, xgb.DMatrix(as.matrix(sub)))
stack_score <- (logit_score + lasso_score + ridge_score + rf_score + xg_score)/5
stacked <- data.frame(article_id = article_ids, score = stack_score)
write_csv(stacked, "./submissions/stacked_submission.csv")
logit_vscore <- predict(logit, valid, type='response')
lasso_vscore <- c(predict(lasso, newx=model.matrix(~., valid)[,-1]))
formula <- as.formula(is_popular ~ .)
X = model.matrix(formula, train)[,-1]
Y = train$is_popular
#fit model
lasso <- cv.glmnet(X, Y, alpha=1, nlambda = 200)
#fit model
lasso <- cv.glmnet(X, Y, alpha=1, nlambda = 200)
#transform fitted values and add to table
accuracy_results <- add_row(accuracy_results,
model = "LASSO",
train = calculateAccuracy(ifelse(predict(lasso, newx = X, s = lasso$lambda.min) < 0.5, 0, 1), train$is_popular),
validation = calculateAccuracy(ifelse(predict(lasso, newx = model.matrix(formula, valid)[,-1], s = lasso$lambda.min) < 0.5, 0, 1), valid$is_popular)
)
lasso_vscore <- c(predict(lasso, newx=model.matrix(~., valid)[,-1], s = lasso$lambda.min))
model.matrix(~., valid)
model.matrix(~., valid)[,-1]
dim(model.matrix(~., valid)[,-1])
dim(model.matrix(~., valid[,-1]))
dim(valid)
formula <- as.formula(is_popular ~ .)
lasso_vscore <- c(predict(lasso, newx=model.matrix(is_popular ~ ., valid)[,-1], s = lasso$lambda.min))
ridge_vscore <- c(predict(ridge, newx=model.matrix(is_popular ~ ., valid)[,-1], s = ridge$lambda.min))
rf_vscore <- predict(rf, data=valid)$predictions
xg_vscore <- predict(xgb, xgb.DMatrix(as.matrix(valid)))
xg_vscore <- predict(xgb, xgb.DMatrix(as.matrix(valid)[,-1]))
dim(as.matrix(valid)[,-1])
xg_vscore <- predict(xgb, xgb.DMatrix(as.matrix(valid)[,-1]))
xg_vscore <- predict(xgb, predict(xgb, xgb_valid)))
xg_vscore <- predict(xgb,  xgb_valid)))
xg_vscore <- predict(xgb,  xgb_valid))
xg_vscore <- predict(xgb,  xgb_valid)
#average out predictions
stack_vscore <- (logit_vscore + lasso_vscore + ridge_vscore + rf_vscore + xg_vscore)/5
accuracy_results <- add_row(accuracy_results,
model = "Stacking",
test = calculateRMSLE(ifelse(stacked_vscore < 0.5, 0, 1), valid$is_popular)
)
accuracy_results <- add_row(accuracy_results,
model = "Stacking",
test = calculateAccuracy(ifelse(stacked_vscore < 0.5, 0, 1), valid$is_popular)
)
accuracy_results <- add_row(accuracy_results,
model = "Stacking",
test = calculateAccuracy(ifelse(stack_vscore < 0.5, 0, 1), valid$is_popular)
)
show_table()
logit_score <- predict(logit, sub, type='response')
lasso_score <- c(predict(lasso, newx=model.matrix(~., sub)[,-1], s = lasso$lambda.min))
ridge_score <- c(predict(ridge, newx=model.matrix(~., sub)[,-1], s = ridge$lambda.min))
rf_score <- predict(rf, data=sub)$predictions
xg_score <- predict(xgb, xgb.DMatrix(as.matrix(sub)))
#average out predictions
stack_score <- (logit_score + lasso_score + ridge_score + rf_score + xg_score)/5
stacked <- data.frame(article_id = article_ids, score = stack_score)
write_csv(stacked, "./submissions/stacked_submission.csv")
# import files in h2o format
work <- h2o.importFile(path = '../data/online_news/train.csv')
h20.init()
h2o.init()
# import files in h2o format
work <- h2o.importFile(path = '../data/online_news/train.csv')
holdout <- h2o.importFile(path = '../data/online_news/test.csv')
# convert outcome to factor
work["is_popular"] <- as.factor(work["is_popular"])
# split dataframe
my_seed <- 1134
news_data_splits <- h2o.splitFrame(data =  work, ratios = 0.8, seed = my_seed)
train <- news_data_splits[[1]]
valid <- news_data_splits[[2]]
# set the predictor and response columns
response <- "is_popular"
predictors <- setdiff(names(train), c(response, "article_id"))
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = train,
validation_frame = valid,
nfolds = 8,
sort_metric = "AUC",
seed = my_seed,
max_models = 50,
max_runtime_secs = 60*10 # limit the run-time
)
automl
h2o.get_best_model(automl)
best_auto_ml <- h2o.get_best_model(automl)
setwd("~/Desktop/repos/ceu-ml/homeworks")
h2o.save_model(model=best_model,path='/h2o/best_automl', force=True)
h2o.saveModel(model=best_model,path='/h2o/best_automl', force=True)
h2o.saveModel(best_auto_ml,path='/h2o/best_automl', force=True)
h2o.saveModel(best_auto_ml,path='/h2o/best_automl', force=TRUE)
h2o.saveModel(best_auto_ml,path='/h2o/best_automl', force=TRUE)
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
View(aml_submission)
h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.load_model(path=modelpath)
best_auto_ml <- h2o.loadModel(path=modelpath)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = train,
validation_frame = valid,
nfolds = 7,
sort_metric = "AUC",
seed = my_seed,
max_models = 20,
max_runtime_secs = 60*10 # limit the run-time
)
automl
best_auto_ml <- h2o.get_best_model(automl)
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml
summary(best_auto_ml)
View(bdata)
View(best_auto_ml)
best_auto_ml@algorithm
best_auto_ml@parameters[["model_id"]]
best_auto_ml@parameters
best_auto_ml@model[["model_summary"]]
best_auto_ml@model[["model_summary"]]
accuracy_results <- add_row(accuracy_results,
model = "h2o AutoML",
train = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, train)[[3]]) < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, valid)[[3]]) < 0.5, 0, 1), valid$is_popular)
)
show_table()
array(h2o.predict(best_auto_ml, valid)[[3]])
valid$is_popular
# got the data from kaggle
work <- read_csv("../data/online_news/train.csv")
holdout <- read_csv("../data/online_news/test.csv")
head(work)
#remove duplicated rows if exist
work <- work |> distinct()
#inspect missing values
work[!complete.cases(work), ]
# find number of rows that contain 0 for n_tokens_content
nrow(work[work['n_tokens_content']==0,])
# 883 rows with no body just title, drop them
work <- work[work['n_tokens_content'] != 0, ]
to_drop <- c("n_non_stop_unique_tokens","n_non_stop_words","kw_avg_min","article_id","timedelta")
# drop useless columns which highly correlate or is not informative
work <- select(work, -all_of(to_drop))
#Data partition to work and test set 80-20 split
set.seed(1134)
ind<-sample(nrow(work),size=floor(0.8*nrow(work)))
train<-work[ind,]
valid<-work[-ind,]
trainh <- news_data_splits[[1]]
validh <- news_data_splits[[2]]
u
accuracy_results <- add_row(accuracy_results,
model = "h2o AutoML",
train = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, trainh)[[3]]) < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, validh)[[3]]) < 0.5, 0, 1), valid$is_popular)
)
show_table()
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
holdout <- h2o.importFile(path = '../data/online_news/test.csv')
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
# import files in h2o format
work <- h2o.importFile(path = '../data/online_news/train.csv')
holdout <- h2o.importFile(path = '../data/online_news/test.csv')
# convert outcome to factor
work["is_popular"] <- as.factor(work["is_popular"])
# split dataframe
my_seed <- 1134
news_data_splits <- h2o.splitFrame(data =  work, ratios = 0.8, seed = my_seed)
trainh <- news_data_splits[[1]]
validh <- news_data_splits[[2]]
# set the predictor and response columns
response <- "is_popular"
predictors <- setdiff(names(trainh), c(response, "article_id"))
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = trainh,
validation_frame = validh,
nfolds = 5,
sort_metric = "AUC",
seed = my_seed,
max_models = 30,
max_runtime_secs = 60*5 # limit the run-time
)
best_auto_ml <- h2o.get_best_model(automl)
best_auto_ml
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml@model[["model_summary"]]
accuracy_results <- add_row(accuracy_results,
model = "h2o AutoML",
train = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, trainh)[[3]]) < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, validh)[[3]]) < 0.5, 0, 1), valid$is_popular)
)
show_table()
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = trainh,
validation_frame = validh,
nfolds = 5,
sort_metric = "AUC",
seed = my_seed,
max_models = 30,
max_runtime_secs = 60*3 # limit the run-time
)
best_auto_ml <- h2o.get_best_model(automl)
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml@model[["model_summary"]]
best_auto_ml@model[["validation_metrics"]]
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml@model[["model_summary"]]
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = trainh,
validation_frame = validh,
nfolds = 5,
sort_metric = "AUC",
seed = my_seed,
max_models = 20,
max_runtime_secs = 60*2 # limit the run-time
)
best_auto_ml <- h2o.get_best_model(automl)
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml@model[["model_summary"]]
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = trainh,
validation_frame = validh,
nfolds = 5,
sort_metric = "AUC",
seed = my_seed,
max_models = 20,
max_runtime_secs = 60*3 # limit the run-time
)
best_auto_ml <- h2o.get_best_model(automl)
model_path <- h2o.saveModel(best_auto_ml,path=getwd(), force=TRUE)
best_auto_ml <- h2o.loadModel(path=model_path)
best_auto_ml@model[["model_summary"]]
# Validation accuracy
accuracy_results <- add_row(accuracy_results,
model = "h2o AutoML",
train = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, trainh)[[3]]) < 0.5, 0, 1), train$is_popular),
test = calculateAccuracy(ifelse(array(h2o.predict(best_auto_ml, validh)[[3]]) < 0.5, 0, 1), valid$is_popular)
)
show_table()
#Submission
aml_score <- array(h2o.predict(best_auto_ml, holdout)[[3]])
aml_submission <-data.frame(article_id = article_ids, score = aml_score)
write_csv(aml_submission, "./submissions/aml_submission.csv")
best_auto_ml
?h2o.automl
# fit automl
automl <- h2o.automl(
x = predictors, y = response,
training_frame = trainh,
validation_frame = validh,
nfolds = 5,
sort_metric = "AUC",
balance_classes = TRUE,
seed = my_seed,
max_models = 20,
max_runtime_secs = 60*3 # limit the run-time
)
