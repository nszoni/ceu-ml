data = caravan_train
)
accuracy_results <- add_row(accuracy_results,
model = "Random Forest",
train = calculateAccuracy(ifelse(predict(rf, caravan_train, na.action = na.omit)$predictions < 0.5, 0, 1), caravan_train$Purchase),
test =calculateAccuracy(ifelse(predict(rf, caravan_test)$predictions < 0.5, 0, 1), caravan_test$Purchase)
)
calculateAccuracy(ifelse(predict(rf, caravan_train, na.action = na.omit)$predictions < 0.5, 0, 1), caravan_train$Purchase)
calculateAccuracy(ifelse(predict(rf, caravan_test)$predictions < 0.5, 0, 1), caravan_test$Purchase)
View(caravan_test)
new_DF <- caravan_test[rowSums(is.na(caravan_test) > 0,]
caravan_test[!complete.cases(caravan_test), ]
ifelse(predict(rf, caravan_test)$predictions < 0.5, 0, 1)
df <- caravan_test[!complete.cases(caravan_test), ]
View(df)
accuracy_results <- add_row(accuracy_results,
model = "Random Forest",
train = calculateAccuracy(ifelse(predict(rf, caravan_train, na.action = na.omit)$predictions < 0.5, 0, 1), caravan_train$Purchase),
test = calculateAccuracy(ifelse(predict(rf, na.omit(caravan_test))$predictions < 0.5, 0, 1), caravan_test$Purchase)
)
calculateAccuracy(ifelse(predict(rf, na.omit(caravan_test))$predictions < 0.5, 0, 1), caravan_test$Purchase)
accuracy_results <- add_row(accuracy_results,
model = "Random Forest",
train = calculateAccuracy(ifelse(predict(rf, caravan_train, na.action = na.omit)$predictions < 0.5, 0, 1), caravan_train$Purchase),
test = calculateAccuracy(ifelse(predict(rf, na.omit(caravan_test))$predictions < 0.5, 0, 1), na.omit(caravan_test)$Purchase)
)
kable(x = accuracy_results, digits = 3, caption = 'Evaluation Metrics') %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
add_footnote(c("Source data: Caravan Insurance Purchase"))
show_table <- function(){
kable(x = accuracy_results, digits = 3, caption = 'Evaluation Metrics') %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
add_footnote(c("Source data: Caravan Insurance Purchase"))
}
show_table()
#CART
caravan_data[,col_names] <- lapply(caravan_data[,col_names], factor)
caravan_train_full <- anti_join(caravan_data, caravan_test)
tree_model_full <- rpart(
Purchase ~ .,
caravan_train_full
)
accuracy_results <- add_row(accuracy_results,
model = "CART Full",
train = calculateAccuracy(ifelse(predict(tree_model_full, caravan_train_full) < 0.5, 0, 1), caravan_train_full$Purchase),
test = calculateAccuracy(ifelse(predict(tree_model_full, caravan_test) < 0.5, 0, 1), caravan_test$Purchase)
)
show_table()
#Regression Tree
rf_full <- ranger(
Purchase ~ .,
caravan_train_full
)
accuracy_results <- add_row(accuracy_results,
model = "Random Forest Full",
train = calculateAccuracy(ifelse(predict(rf_full, caravan_train_full)$predictions < 0.5, 0, 1), caravan_train_full$Purchase),
test = calculateAccuracy(ifelse(predict(rf_full, caravan_test)$predictions < 0.5, 0, 1), caravan_test$Purchase)
)
accuracy_results <- add_row(accuracy_results,
model = "Random Forest Full",
train = calculateAccuracy(ifelse(predict(rf_full, caravan_train_full)$predictions < 0.5, 0, 1), caravan_train_full$Purchase),
test = calculateAccuracy(ifelse(predict(rf_full, na.omit(caravan_test))$predictions < 0.5, 0, 1), na.omit(caravan_test)$Purchase)
)
show_table()
# confusion matrix for the train set
cm <- table(ifelse(predict(rf_full, caravan_test) < 0.5, 0, 1), caravan_test$Purchase)
# confusion matrix for the train set
cm <- table(ifelse(predict(rf_full, na.omit(caravan_test)) < 0.5, 0, 1), na_omit(caravan_test)$Purchase)
# confusion matrix for the train set
cm <- table(ifelse(predict(rf_full, na.omit(caravan_test))$predictions < 0.5, 0, 1), na_omit(caravan_test)$Purchase)
# confusion matrix for the train set
cm <- table(ifelse(predict(rf_full, na.omit(caravan_test))$predictions < 0.5, 0, 1), na.omit(caravan_test)$Purchase)
kable(x = cm, digits = 3, caption = 'Confusion Matrix Train Set (Purchase)') %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
add_footnote(c("w/ Logit on full set of variables")) %>%
add_header_above(., header = c("Predicted" = 1, "Reference" = 2))
predict(rf_full, na.omit(caravan_test))$predictions < 0.5, 0, 1)
predict(rf_full, na.omit(caravan_test))$predictions
# confusion matrix for the train set
cm <- table(ifelse(predict(rf_full, na.omit(caravan_test))$predictions < 0.5, 0, 1), na.omit(caravan_test)$Purchase)
kable(x = cm, digits = 3, caption = 'Confusion Matrix Train Set (Purchase)') %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
add_footnote(c("w/ Logit on full set of variables")) %>%
add_header_above(., header = c("Predicted" = 1, "Reference" = 2))
#Loss function
calculateRMSLE <- function(prediction, y_obs) {
sqrt(mean((log(ifelse(prediction < 0, 0, prediction) + 1) - log(y_obs + 1))^2))
}
#Loss function
calculateRMSLE <- function(prediction, y_obs) {
sqrt(mean((log(ifelse(prediction < 0, 0, prediction) + 1) - log(y_obs + 1))^2))
}
real_estate <- read_csv("../data/real_estate/real_estate.csv")
set.seed(1234)
n_obs <- nrow(real_estate)
test_share <- 0.2
test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
real_estate_test <- slice(real_estate, test_indices)
real_estate_train <- slice(real_estate, -test_indices)
real_estate <- read_csv("../data/real_estate/real_estate.csv")
set.seed(1234)
n_obs <- nrow(real_estate)
test_share <- 0.2
test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
real_estate_test <- slice(real_estate, test_indices)
real_estate_train <- slice(real_estate, -test_indices)
lm <- lm(house_price_of_unit_area ~ ., data = real_estate_train)
rmsle_results <- tibble(
model = "Random Forest",
train = calculateRMSLE(predict(lm, real_estate_train), real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(lm, real_estate_test), real_estate_test$house_price_of_unit_area)
)
lm <- lm(house_price_of_unit_area ~ ., data = real_estate_train)
rmsle_results <- tibble(
model = "Linear Regression",
train = calculateRMSLE(predict(lm, real_estate_train), real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(lm, real_estate_test), real_estate_test$house_price_of_unit_area)
)
# create reusable table display
show_table <- function(){
kable(x = rmsle_results, digits = 3, caption = 'Evaluation Metrics (RMSLE)') %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
add_footnote(c("Source data: Real Estate Value"))
}
show_table()
#Regression Tree
rf_re <- ranger(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station + km_from_center*number_of_convenience_stores,
real_estate_train
)
#calculate distance form New Taipei City Hall
#Vector of distances in the same unit as r (default is meters)
library(geosphere)
centroid <- c(121.5654, 25.0330)
vect <- c()
for (i in 1:nrow(real_estate)){
v <- distm(c(as.numeric(real_estate[i, 'longitude']), as.numeric(real_estate[i, 'latitude'])), centroid, fun = distHaversine)/1000
vect <- c(vect, v)
}
#assign distance to new column
real_estate$km_from_center <- as.integer(vect)
set.seed(1234)
n_obs <- nrow(real_estate)
test_share <- 0.2
test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
real_estate_test <- slice(real_estate, test_indices)
real_estate_train <- slice(real_estate, -test_indices)
#Regression Tree
rf_re <- ranger(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station + km_from_center*number_of_convenience_stores,
real_estate_train
)
rmsle_results <- add_row(rmsle_results,
model = "Random Forest",
train = calculateRMSLE(predict(rf_re, real_estate_train)$predictions, real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(rf_re, real_estate_test)$predictions, real_estate_test$house_price_of_unit_area)
)
predict(rf_re, real_estate_train)$predictions
predict(rf_re, real_estate_train)
predict(rf_re, real_estate_train)
#Regression Tree
rf_re <- ranger(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
real_estate_train
)
predict(rf_re, real_estate_train)
#Regression Tree
rf_re <- ranger(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
real_estate_train
)
rmsle_results <- add_row(rmsle_results,
model = "Random Forest",
train = calculateRMSLE(predict(rf_re, real_estate_train)$predictions, real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(rf_re, real_estate_test)$predictions, real_estate_test$house_price_of_unit_area)
)
show_table()
#GBM
gbm <- gbm(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
data = real_estate_train,
n.trees = 1000,
shrinkage = 0.01,
interaction.depth = 4
)
rmsle_results <- add_row(rmsle_results,
model = "GBM",
train = calculateRMSLE(predict(gbm, real_estate_train),real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(gbm, real_estate_test), real_estate_test$house_price_of_unit_area)
)
show_table()
?gbm
#GBM
gbm <- gbm(house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
data = real_estate_train,
n.trees = 1000,
shrinkage = 0.01,
interaction.depth = 2
)
rmsle_results <- add_row(rmsle_results,
model = "GBM",
train = calculateRMSLE(predict(gbm, real_estate_train),real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(gbm, real_estate_test), real_estate_test$house_price_of_unit_area)
)
show_table()
#CART
tree_model <- rpart(
Purchase ~ house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station + km_from_center*number_of_convenience_stores,
real_estate_train
)
#CART
tree_model <- rpart(
house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station + km_from_center*number_of_convenience_stores,
real_estate_train
)
#CART
tree_model <- rpart(
house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
real_estate_train
)
rmsle_results <- add_row(rmsle_results,
model = "CART",
train = calculateAccuracy(ifelse(predict(tree_model, real_estate_train) < 0.5, 0, 1), caravan_train$Purchase),
test = calculateAccuracy(ifelse(predict(tree_model, real_estate_test) < 0.5, 0, 1), caravan_test$Purchase)
)
show_table()
#CART
tree_model <- rpart(
house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
real_estate_train
)
rmsle_results <- add_row(rmsle_results,
model = "CART",
train = calculateRMSLE(predict(tree_model, real_estate_train)$predictions, real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(tree_model, real_estate_test)$predictions, real_estate_test$house_price_of_unit_area)
)
#CART
tree_model <- rpart(
house_price_of_unit_area ~ house_age + house_age^2 + km_from_center + number_of_convenience_stores + distance_to_the_nearest_MRT_station,
real_estate_train
)
rmsle_results <- add_row(rmsle_results,
model = "CART",
train = calculateRMSLE(predict(tree_model, real_estate_train), real_estate_train$house_price_of_unit_area),
test = calculateRMSLE(predict(tree_model, real_estate_test), real_estate_test$house_price_of_unit_area)
)
show_table()
#Stacking
#gather predictions in a tibble
real_estate_predictions <- select(real_estate_test, house_price_of_unit_area) |>
mutate(
prediction_lm = predict(lm, real_estate_test),
prediction_tree = predict(tree_model, real_estate_test),
prediction_rf = predict(rf_re, real_estate_test)$predictions,
prediction_gbm = predict(gbm, real_estate_test)$predictions
)
predict(gbm, real_estate_test)
predict(rf_re, real_estate_test)
#Stacking
#gather predictions in a tibble
real_estate_predictions <- select(real_estate_test, house_price_of_unit_area) |>
mutate(
prediction_lm = predict(lm, real_estate_test),
prediction_tree = predict(tree_model, real_estate_test),
prediction_rf = predict(rf_re, real_estate_test)$predictions,
prediction_gbm = predict(gbm, real_estate_test)
)
View(real_estate_predictions)
#Stacking
#gather predictions in a tibble
real_estate_predictions <- select(real_estate_test, house_price_of_unit_area) |>
mutate(
prediction_lm = predict(lm, real_estate_test),
prediction_tree = predict(tree_model, real_estate_test),
prediction_rf = predict(rf_re, real_estate_test)$predictions,
prediction_gbm = predict(gbm, real_estate_test)
)
#average out predictions
stacked_prediction <- (real_estate_predictions$prediction_lm + real_estate_predictions$prediction_tree + real_estate_predictions$prediction_rf + real_estate_predictions$prediction_gbm) / 4
rmsle_results <- add_row(rmsle_results,
model = "Stacking",
test = calculateRMSLE(stacked_prediction, real_estate_predictions$house_price_of_unit_area)
)
show_table()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, messages = FALSE)
# Loading packages with pacman
if (!require("pacman")) {
install.packages("pacman")
}
pacman::p_load(tidyverse, glmnet, pls, rpart, ranger, gbm, kableExtra)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, messages = FALSE)
real_estate <- read_csv("../data/real_estate/real_estate.csv")
set.seed(1234)
n_obs <- nrow(real_estate)
test_share <- 0.2
test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
real_estate_test <- slice(real_estate, test_indices)
real_estate_train <- slice(real_estate, -test_indices)
library(tidyverse)
theme_set(theme_minimal())
library(h2o)
h2o.init()
# import the prostate dataset
data_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"
prostate_data <- h2o.importFile(data_url)
# convert columns to factors (we cannot use the tidyverse functions here as the object is not a normal df)
prostate_data$CAPSULE <- as.factor(prostate_data$CAPSULE)
prostate_data$RACE <- as.factor(prostate_data$RACE)
prostate_data$DCAPS <- as.factor(prostate_data$DCAPS)
prostate_data$DPROS <- as.factor(prostate_data$DPROS)
summary(prostate_data)
my_seed <- 20220316
prostate_data_splits <- h2o.splitFrame(data =  prostate_data, ratios = 0.8, seed = my_seed)
train <- prostate_data_splits[[1]]
test <- prostate_data_splits[[2]]
simple_xgboost <- h2o.xgboost(
x = predictors, y = response,
model_id = "simple_xgboost",
training_frame = train,
validation_frame = test,
nfolds = 5,
score_each_iteration = TRUE,
seed = my_seed
)
# set the predictor and response columns
response <- "CAPSULE"
predictors <- setdiff(names(prostate_data), c(response, 'ID'))
# build a simple GLM model using CV (just for evaluation, no hyperparam tuning yet)
simple_logit <- h2o.glm(
family = "binomial",
model_id = "logit",
x = predictors,
y = response,
training_frame = train,
lambda = 0,
nfolds = 5
)
simple_xgboost <- h2o.xgboost(
x = predictors, y = response,
model_id = "simple_xgboost",
training_frame = train,
validation_frame = test,
nfolds = 5,
score_each_iteration = TRUE,
seed = my_seed
)
h2o.xgboost.available()
install.packages('DALEX')
install.packages('DALEXtra')
install.packages('lime')
library(tidyverse)
library(h2o)
library(DALEX)
library(DALEXtra)
clean_data <- read_csv("../data/airbnb_gabors-data/airbnb_london_workfile_adj_book.csv")
basic_vars <- c(
"n_accommodates", "n_beds", "n_days_since",
"f_property_type","f_room_type", "f_bathroom", "f_cancellation_policy", "f_bed_type",
"f_neighbourhood_cleansed"
)
amenities <-  grep("^d_.*", names(clean_data), value = TRUE)
working_data <- clean_data |>
filter(
!is.na(price),
n_accommodates < 8,
flag_review_scores_rating == 0
) |>
select(price, any_of(c("n_review_scores_rating", basic_vars, amenities))) |>
mutate(across(starts_with(c("d_", "f_")), factor))
outcome_variable <- "price"
features <- setdiff(names(working_data), outcome_variable)
h2o.init()
my_seed <- 20220330
data_split <- h2o.splitFrame(as.h2o(working_data), ratios = 0.75, seed = my_seed)
airbnb_train <- data_split[[1]]
airbnb_holdout <- data_split[[2]]
linear_model <- h2o.glm(
features, outcome_variable,
training_frame = airbnb_train,
validation_frame = airbnb_holdout,
alpha = 1,  # lasso
seed = my_seed
)
h2o.coef(linear_model)[h2o.coef(linear_model) > 0]
h2o.performance(linear_model, valid = TRUE)
rf_model <- h2o.randomForest(
features, outcome_variable,
training_frame = airbnb_train,
validation_frame = airbnb_holdout,
seed = my_seed
)
h2o.performance(rf_model, valid = TRUE)
h2o.varimp(rf_model)
h2o.varimp_plot(rf_model, num_of_features = 20)
explainer_rf <- explain_h2o(rf_model, data = airbnb_holdout[features], y = airbnb_holdout[[outcome_variable]])
class(explainer_rf)
summary(explainer_rf)
pdp_rf <- model_profile(explainer_rf, variable_type = "numerical")  # takes a while...
plot(pdp_rf)
plot(pdp_rf, geom = "points")
setwd("~/Desktop/repos/ceu-ml/tools")
library(tidyverse)
library(h2o)
library(DALEX)
library(DALEXtra)
theme_set(theme_minimal())
clean_data <- read_csv("../data/airbnb_gabors-data/airbnb_london_workfile_adj_book.csv")
basic_vars <- c(
"n_accommodates", "n_beds", "n_days_since",
"f_property_type","f_room_type", "f_bathroom", "f_cancellation_policy", "f_bed_type",
"f_neighbourhood_cleansed"
)
amenities <-  grep("^d_.*", names(clean_data), value = TRUE)
working_data <- clean_data |>
filter(
!is.na(price),
n_accommodates < 8,
flag_review_scores_rating == 0
) |>
select(price, any_of(c("n_review_scores_rating", basic_vars, amenities))) |>
mutate(across(starts_with(c("d_", "f_")), factor))
outcome_variable <- "price"
features <- setdiff(names(working_data), outcome_variable)
h2o.init()
my_seed <- 20220330
data_split <- h2o.splitFrame(as.h2o(working_data), ratios = 0.75, seed = my_seed)
airbnb_train <- data_split[[1]]
airbnb_holdout <- data_split[[2]]
linear_model <- h2o.glm(
features, outcome_variable,
training_frame = airbnb_train,
validation_frame = airbnb_holdout,
alpha = 1,  # lasso
seed = my_seed
)
h2o.coef(linear_model)[h2o.coef(linear_model) > 0]
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer
lime_explanation <- predict_surrogate(
explainer = explainer_rf,
new_observation = obs_of_interest,  # needs to use a normal df not an H2OFrame!
type = "lime",
n_features = 10,  # default: 4
seed = my_seed  # samples for permutations - still not reproducible :(
)
obs_of_interest <- as_tibble(airbnb_holdout)[4105, features]
obs_of_interest
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer
lime_explanation <- predict_surrogate(
explainer = explainer_rf,
new_observation = obs_of_interest,  # needs to use a normal df not an H2OFrame!
type = "lime",
n_features = 10,  # default: 4
seed = my_seed  # samples for permutations - still not reproducible :(
)
plot(lime_explanation)
model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer
lime_explanation <- predict_surrogate(
explainer = explainer_rf,
new_observation = obs_of_interest,  # needs to use a normal df not an H2OFrame!
type = "lime",
n_features = 10,  # default: 4
seed = my_seed  # samples for permutations - still not reproducible :(
)
plot(lime_explanation)
treatment_variable <- "d_airconditioning"
pdp_variable <- model_profile(explainer_rf, variables = treatment_variable)
pdp_variable
plot(model_profile(explainer_rf, variables = treatment_variable))
plot(model_profile(explainer_rf, variables = "n_review_scores_rating", groups = treatment_variable)) +
xlim(80, 100)
features_base <- setdiff(features, treatment_variable)
g_h2o <- h2o.randomForest(  # regression problem
x = features_base, y = outcome_variable,
training_frame = airbnb_train,
seed = my_seed
)
h2o.performance(g_h2o, airbnb_holdout)
m_h2o <- h2o.xgboost(  # classification problem
x = features_base, y = treatment_variable,
training_frame = airbnb_train,
validation_frame = airbnb_holdout,
seed = my_seed
)
source("bandit-functions.R")
VERSION_PROBS <- c(0.1, 0.3, 0.5, 0.7)
n_sim <- 100
etc10_results <- runSimulations(n_sim, "ETC", VERSION_PROBS, policy_params = list(explore_until = 10))
etc30_results <- runSimulations(n_sim, "ETC", VERSION_PROBS, policy_params = list(explore_until = 30))
etc50_results <- runSimulations(n_sim, "ETC", VERSION_PROBS, policy_params = list(explore_until = 50))
etc70_results <- runSimulations(n_sim, "ETC", VERSION_PROBS, policy_params = list(explore_until = 70))
eg10_results <- runSimulations(n_sim, "epsGreedy", VERSION_PROBS, policy_params = list(epsilon = 0.1))
eg30_results <- runSimulations(n_sim, "epsGreedy", VERSION_PROBS, policy_params = list(epsilon = 0.3))
eg50_results <- runSimulations(n_sim, "epsGreedy", VERSION_PROBS, policy_params = list(epsilon = 0.5))
eg70_results <- runSimulations(n_sim, "epsGreedy", VERSION_PROBS, policy_params = list(epsilon = 0.7))
ucb_results <- runSimulations(n_sim, "UCB", VERSION_PROBS)
model_results <- bind_rows(
mutate(etc10_results, policy = "ETC10"),
mutate(etc30_results, policy = "ETC30"),
mutate(etc50_results, policy = "ETC50"),
mutate(etc70_results, policy = "ETC70"),
mutate(eg10_results, policy = "EG10"),
mutate(eg30_results, policy = "EG30"),
mutate(eg50_results, policy = "EG50"),
mutate(eg70_results, policy = "EG70"),
mutate(ucb_results, policy = "UCB")
)
View(model_results)
ggplot(model_results, aes(x = i, y = showed_version, group = run)) +
geom_line(alpha = 0.1, color = "navy") +
facet_wrap(~ policy)
model_results |>
group_by(policy, run) |>
summarize(conversion_rate = mean(conversion)) |>
ggplot(aes(policy, conversion_rate)) + geom_boxplot()
